###############################################################################
#
# Licensed Materials - Property of IBM
#
# (C) Copyright IBM Corp. 2021. All Rights Reserved.
#
# US Government Users Restricted Rights - Use, duplication or
# disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
#
###############################################################################
apiVersion: icp4a.ibm.com/v1
kind: ICP4ACluster
metadata:
  name: workstreams
  labels:
    app.kubernetes.io/instance: ibm-dba
    app.kubernetes.io/managed-by: ibm-dba
    app.kubernetes.io/name: ibm-dba
    release: 21.0.1
spec:
  appVersion: 21.0.1.1

  ## MUST exist, used to accept ibm license, valid value only can be "accept"
  ibm_license: ""

  ##########################################################################
  ## This section contains the shared configuration for all CP4A components #
  ##########################################################################
  shared_configuration:

    ## Business Automation Workflow (BAW) license and possible values are: user, non-production, and production.
    ## This value could be different from the other licenses in the CR.
    sc_deployment_baw_license: "<Required>"

    ## FileNet Content Manager (FNCM) license and possible values are: user, non-production, and production.
    ## This value could be different from the other licenses in the CR.
    sc_deployment_fncm_license: "<Required>"

    ## Use this parameter to specify the license for the CP4A deployment and
    ## the possible values are: non-production and production and if not set, the license will
    ## be defaulted to production.  This value could be different from the other licenses in the CR.
    sc_deployment_license: "<Required>"

    ## All CP4A components must use/share the image_pull_secrets to pull images
    image_pull_secrets:
    - admin.registrykey

    ## All CP4A components must use/share the same docker image repository.  For example, if IBM Entitled Registry is used, then
    ## it should be "cp.icr.io".  Otherwise, it will be a local docker registry.
    sc_image_repository: cp.icr.io

    ## For non-OCP (e.g., CNCF platforms such as AWS, GKE, etc), this parameter is required. Not supported on OCP and ROKS.
    sc_run_as_user:

    images:
      keytool_job_container:
        repository: cp.icr.io/cp/cp4a/baw/dba-keytool-jobcontainer
        tag: 21.0.1
      dbcompatibility_init_container:
        repository: cp.icr.io/cp/cp4a/baw/dba-dbcompatibility-initcontainer
        tag: 21.0.1
      keytool_init_container:
        repository: cp.icr.io/cp/cp4a/baw/dba-keytool-initcontainer
        tag: 21.0.1
      umsregistration_initjob:
        repository: cp.icr.io/cp/cp4a/baw/dba-umsregistration-initjob
        tag: 21.0.1

      ## All CP4A components should use this pull_policy as the default, but it can override by each component
      pull_policy: IfNotPresent

    ## All CP4A components must use/share the root_ca_secret in order for integration
    root_ca_secret: icp4ba-root-ca

    ## CP4A patterns or capabilities to be deployed.  This CR represents the "workstreams" pattern, which includes the following
    ## mandatory components: ban(Business Automation Navigator), ums (User Management Service), rr (Resource registry), app_engine( Application Engine)
    sc_deployment_patterns: workstreams

    ## The optional components to be installed if listed here.  This is normally populated by the User script based on input from the user.
    ## The optional components are: bai,ae_data_persistence
    sc_optional_components: ae_data_persistence

    ## The deployment type as selected by the user.  Possible values are: demo, enterprise
    sc_deployment_type: enterprise

    ## The platform to be deployed specified by the user.  Possible values are: OCP and other.  This is normally populated by the User script
    ## based on input from the user.
    sc_deployment_platform:

    ## For ROKS, this is used to enable the creation of ingresses. The default value is "false", which routes will be created.
    sc_ingress_enable: false

    ## For ROKS Ingress, provide TLS secret name for Ingress controller.
    sc_ingress_tls_secret_name: <Required>

    ## For OCP, this is used to create route, you should input a valid hostname in the required field.
    sc_deployment_hostname_suffix: "{{ meta.namespace }}.<Required>"

    ## If the root certificate authority (CA) key of the external service is not signed by the operator root CA key, provide the TLS certificate of
    ## the external service to the component's truststore.
    trusted_certificate_list: []

    ## Shared encryption key secret name that is used for Workflow or Workstream Services and Process Federation Server integration.
    ## This secret is also used by Workflow and BAStudio to store AES encryption key.
    encryption_key_secret: ibm-iaws-shared-key-secret

    ## On OCP 3.x and 4.x, the User script will populate these three (3) parameters based on your input for "enterprise" deployment.
    ## If you manually deploying without using the User script, then you would provide the different storage classes for the slow, medium
    ## and fast storage parameters below.  If you only have 1 storage class defined, then you can use that 1 storage class for all 3 parameters.
    storage_configuration:
      sc_slow_file_storage_classname: "<Required>"
      sc_medium_file_storage_classname: "<Required>"
      sc_fast_file_storage_classname: "<Required>"

  ## The beginning section of database configuration for CP4A
  datasource_configuration:
    ## The dc_ssl_enabled parameter is used to support database connection over SSL for DB2/Oracle.
    dc_ssl_enabled: true
    ## The database_precheck parameter is used to enable or disable CPE/Navigator database connection check.
    ## If set to "true", then CPE/Navigator database connection check will be enabled.
    ## if set to "false", then CPE/Navigator database connection check will not be enabled.
   # database_precheck: true
    ## The database configuration for the GCD datasource for CPE
    dc_gcd_datasource:
      ## Provide the database type from your infrastructure.  The possible values are "db2" or "db2HADR" or "oracle".
      dc_database_type: "<Required>"
      ## The GCD non-XA datasource name.  The default value is "FNGCDDS".
      dc_common_gcd_datasource_name: "FNGCDDS"
      ## The GCD XA datasource name. The default value is "FNGCDDSXA".
      dc_common_gcd_xa_datasource_name: "FNGCDDSXA"
      ## Provide the database server name or IP address of the database server.
      database_servername: "<Required>"
      ## Provide the name of the database for the GCD for CPE.  For example: "GCDDB"
      database_name: "<Required>"
      ## Provide the database server port.  For Db2, the default is "50000".  For Oracle, the default is "1521"
      database_port: "<Required>"
      ## The name of the secret that contains the DB2 SSL certificate.
      database_ssl_secret_name: "<Required>"
      ## If the database type is Oracle, provide the Oracle DB connection string.  For example, "jdbc:oracle:thin:@//<oracle_server>:1521/orcl"
      dc_oracle_gcd_jdbc_url: "<Required>"

      ## If the database type is Db2 HADR, then complete the rest of the parameters below.
      ## Provide the database server name or IP address of the standby database server.
      dc_hadr_standby_servername: "<Required>"
      ## Provide the standby database server port.  For Db2, the default is "50000".
      dc_hadr_standby_port: "<Required>"
      ## Provide the validation timeout.  If not preference, keep the default value.
      dc_hadr_validation_timeout: 15
      ## Provide the retry internal.  If not preference, keep the default value.
      dc_hadr_retry_interval_for_client_reroute: 15
      ## Provide the max # of retries.  If not preference, keep the default value.
      dc_hadr_max_retries_for_client_reroute: 3

    ## The database configuration for the document object store (DOCS) datasource for CPE
    dc_os_datasources:
    ## The database configuration for the target object store (AWS DOCS) datasource for CPE. Provide the database type from your infrastructure.  The possible values are "db2" or "db2HADR" or "oracle".  This should be the same as the GCD.
    - dc_database_type: "<Required>"
      ## Provide the object store label for the object store.  The default value is "os" or not defined.
      ## This label must match the OS secret you define in ibm-fncm-secret.
      ## For example, if you define dc_os_label: "abc", then your OS secret must be defined as:
      ## --from-literal=abcDBUsername="<your os db username>" --from-literal=abcDBPassword="<your os db password>"
      ## If you don't define dc_os_label, then your secret will be defined as:
      ## --from-literal=osDBUsername="<your os db username>" --from-literal=osDBPassword="<your os db password>".
      ## If you have multiple object stores, then you need to define multiple datasource sections starting
      ## at "dc_database_type" element.
      ## If all the object store databases share the same username and password, then dc_os_label value should be the same
      ## in all the datasource sections.
      dc_os_label: "<Required>"
      ## The DOCS non-XA datasource name.  The default value is "AWSINS1DOCS".
      dc_common_os_datasource_name: "AWSINS1DOCS"
      ## The DOCS XA datasource name.  The default value is "AWSINS1DOCSXA".
      dc_common_os_xa_datasource_name: "AWSINS1DOCSXA"
      ## Provide the database server name or IP address of the database server.  This should be the same as the
      ## GCD configuration above.
      database_servername: "<Required>"
      ## Provide the name of the database for the object store 1 for CPE.  For example: "OS1DB"
      database_name: "<Required>"
      ## Provide the database server port.  For Db2, the default is "50000".  For Oracle, the default is "1521"
      database_port: "<Required>"
      ## The name of the secret that contains the DB2 SSL certificate.
      database_ssl_secret_name: "<Required>"
      ## If the database type is Oracle, provide the Oracle DB connection string.  For example, "jdbc:oracle:thin:@//<oracle_server>:1521/orcl"
      dc_oracle_os_jdbc_url: "<Required>"
      ######################################################################################
      ## If the database type is "Db2HADR", then complete the rest of the parameters below.
      ## Otherwise, remove or comment out the rest of the parameters below.
      ######################################################################################
      dc_hadr_standby_servername: "<Required>"
      ## Provide the standby database server port.  For Db2, the default is "50000".
      dc_hadr_standby_port: "<Required>"
      ## Provide the validation timeout.  If not preference, keep the default value.
      dc_hadr_validation_timeout: 15
      ## Provide the retry internal.  If not preference, keep the default value.
      dc_hadr_retry_interval_for_client_reroute: 15
      ## Provide the max # of retries.  If not preference, keep the default value.
      dc_hadr_max_retries_for_client_reroute: 3
    ## object store for AEOS
    - dc_database_type: "<Required>"
      ## Provide the object store label for the object store.  The default value is "os" or not defined.
      ## This label must match the OS secret you define in ibm-fncm-secret.
      ## For example, if you define dc_os_label: "abc", then your OS secret must be defined as:
      ## --from-literal=abcDBUsername="<your os db username>" --from-literal=abcDBPassword="<your os db password>"
      ## If you don't define dc_os_label, then your secret will be defined as:
      ## --from-literal=osDBUsername="<your os db username>" --from-literal=osDBPassword="<your os db password>".
      ## If you have multiple object stores, then you need to define multiple datasource sections starting
      ## at "dc_database_type" element.
      ## If all the object store databases share the same username and password, then dc_os_label value should be the same
      ## in all the datasource sections.
      dc_os_label: "<Required>"
      ## The AEOS non-XA datasource name.  The default value is "AEOS".
      dc_common_os_datasource_name: "AEOS"
      ## The AEOS XA datasource name.  The default value is "AEOSXA".
      dc_common_os_xa_datasource_name: "AEOSXA"
      ## Provide the database server name or IP address of the database server.  This should be the same as the
      ## GCD configuration above.
      database_servername: "<Required>"
      ## Provide the name of the database for the object store AEOS for CPE.  For example: "AEOSDB"
      database_name: "<Required>"
      ## Provide the database server port.  For Db2, the default is "50000".  For Oracle, the default is "1521"
      database_port: "<Required>"
      ## The name of the secret that contains the DB2 SSL certificate.
      database_ssl_secret_name: "<Required>"
      ## If the database type is Oracle, provide the Oracle DB connection string.  For example, "jdbc:oracle:thin:@//<oracle_server>:1521/orcl"
      dc_oracle_os_jdbc_url: "<Required>"
      ######################################################################################
      ## If the database type is "Db2HADR", then complete the rest of the parameters below.
      ## Otherwise, remove or comment out the rest of the parameters below.
      ######################################################################################
      dc_hadr_standby_servername: "<Required>"
      ## Provide the standby database server port.  For Db2, the default is "50000".
      dc_hadr_standby_port: "<Required>"
      ## Provide the validation timeout.  If not preference, keep the default value.
      dc_hadr_validation_timeout: 15
      ## Provide the retry internal.  If not preference, keep the default value.
      dc_hadr_retry_interval_for_client_reroute: 15
      ## Provide the max # of retries.  If not preference, keep the default value.
      dc_hadr_max_retries_for_client_reroute: 3

  ########################################################################
  ########   IBM Business Automation Workflow configuration     ########
  ########################################################################
  baw_configuration:
  ## The baw_configuration is a list. You can deploy multiple instances of Workflow Server and assign different configurations for each instance.
  ## For each instance, baw_configuration.name and baw_configuration.name.hostname must have different values.
  - name: awsins1
    ## Workflow Server service type.
    service_type: "Route"
    ## Workflow Server hostname.
    hostname: ""
    ## Workflow Server port.
    port: 443
    ## Workflow Server node port.
    nodeport: 30026
    ## Workflow Server environment type. Possible values are Development, Test, Staging, and Production.
    env_type: "Production"
    ## Workflow Server capability.
    capabilities: "workstreams"
    ## Workflow Server replica count.
    replicas: 2
    ## Designate an existing LDAP user for the Workflow Server admin user.
    admin_user: "<Required>"
    ## The name of Workflow Server admin secret. This secret name is optional, if the secret name is null, default secret named {{ meta.name }}-<instance-name>-baw-admin-secret will be generated.
    admin_secret_name: "{{ meta.name }}-instance1-baw-admin-secret"
    ## Whether to use the built-in monitoring capability.
    monitor_enabled: false

    ## Required if you implemented your own portal. For example, https://portal.mycompany.com.
    customized_portal_endpoint: ""

    federated_portal:
      ## Content security policy additional origins for federating on premise BAW systems. E.g ["https://on-prem-baw1","https://on-prem-baw2"]
      content_security_policy_additional_origins: []
    ## External connection timeout.
    external_connection_timeout: ""

    ## The secret that contains the Transport Layer Security (TLS) key and certificate for external https visits. You can enter the secret name here.
    ## If you do not want to use the customized external TLS certificate, leave it empty.
    external_tls_secret:
    ## Certificate authority (CA) used to sign the external TLS secret. It is stored in the secret with the TLS key and certificate. You can enter the secret name here.
    ## If you don't want to use the customized CA to sign the external TLS certificate, leave it empty.
    external_tls_ca_secret:

    tls:
      ## Workflow Server TLS secret that contains tls.key and tls.crt.
      tls_secret_name: ibm-baw-tls
      ## Workflow Server TLS trust list.
      ## You might specify a list of secrets, every secret stores a trusted CA
      ## Use command `kubectl create secret generic baw_custom_trust_ca_secret1 --from-file=tls.crt=./ca1.crt` to generate the secret.
      tls_trust_list:
      ## Secret to store your custom trusted keystore (optional). The type for the keystore must be JKS or PKCS12. All certificates from the keystore are imported into the trust keystore of the Workflow Server.
      ## You might run the following sample command to create the secret:
      ## `kubectl create secret generic baw_custom_trusted_keystore_secret --from-file=truststorefile=./trust.p12 --from-literal=type=PKCS12  --from-literal=password=WebAS`
      tls_trust_store:
    image:
      ## Workflow Server (Process Server) image repository URL.
      repository: cp.icr.io/cp/cp4a/baw/workflow-server
      ## Image tag for Workflow Server container.
      tag: 21.0.1-IF001
      ## Pull policy for Workflow Server container. Default value is IfNotPresent. Possible values are IfNotPresent, Always.
      pullPolicy: IfNotPresent
    pfs_bpd_database_init_job:
      ## Database initialization image repository URL for Process Federation Server.
      repository: cp.icr.io/cp/cp4a/baw/pfs-bpd-database-init-prod
      ## Database initialization image repository tag for Process Federation Server.
      tag: 21.0.1-IF001
      ## Pull policy for Process Federation Server database initialization image. Default value is IfNotPresent. Possible values are IfNotPresent, Always.
      pullPolicy: IfNotPresent
    upgrade_job:
      ## Workflow Server database handling image repository URL.
      repository: cp.icr.io/cp/cp4a/baw/workflow-server-dbhandling
      ## Workflow Server database handling image repository tag.
      tag: 21.0.1-IF001
      ## Pull policy for database handling. Default value is IfNotPresent. Possible values are IfNotPresent, Always.
      pullPolicy: IfNotPresent
    bas_auto_import_job:
      ## Workflow Server Business Automation Studio toolkit init image repository URL.
      repository: cp.icr.io/cp/cp4a/baw/toolkit-installer
      ## Workflow Server Business Automation Studio toolkit init image repository tag.
      tag: 21.0.1-IF001
      ## Pull policy for Workflow Server Business Automation Studio toolkit init image. Default value is IfNotPresent. Possible values are IfNotPresent, Always.
      pullPolicy: IfNotPresent
    ibm_workplace_job:
      ## Workflow Server IBM Workplace deployment job image repository URL
      repository: cp.icr.io/cp/cp4a/baw/iaws-ibm-workplace
      ## Workflow Server IBM Workplace deployment job image repository tag.
      tag: 21.0.1-IF001
      ## Pull policy for Workflow Server IBM Workplace deployment job image. Default value is IfNotPresent. Possible values are IfNotPresent, Always.
      pull_policy: IfNotPresent

    ## The database configuration for Workflow Server
    database:
      ## Whether to enable Secure Sockets Layer (SSL) support for the Workflow Server database connection.
      enable_ssl: true
      ## Secret name for storing the database TLS certificate when an SSL connection is enabled. Required only when enable_ssl is true
      db_cert_secret_name: "<Required>"
      ## Workflow Server database type. Possible values are: db2, oracle, postgresql
      type: "<Required>"
      ## Workflow Server database server name. This parameter is required.
      server_name: "<Required>"
      ## Workflow Server database name. This parameter is required.
      database_name: "<Required>"
      ## Workflow Server database port. This parameter is required. For DB2, the default value is "50000"
      port: "<Required>"
      ## Workflow Server database secret name. This parameter is required.
      ## apiVersion: v1
      ## kind: Secret
      ## metadata:
      ##   name: ibm-baw-wfs-server-db-secret
      ## type: Opaque
      ## data:
      ##   dbUser: <DB_USER>
      ##   password: <DB_USER_PASSWORD>
      secret_name: "<Required>"
      ## Oracle and PostgreSQL database connection string. 
      ## If the database type is Oracle, provide the Oracle database connection string. For example, jdbc:oracle:thin:@//<oracle_server>:1521/orcl. 
      ## If the database type is PostgreSQL, this parameter is optional. For example, jdbc:postgresql://<postgresql_server>:5432/<your_database>. This parameter is not required for PostgreSQL if you enter server_name, database_name, and port. If you do not need this parameter, remove it or comment it out.
      ## In all other cases, remove or comment out this parameter.
      jdbc_url: "<Required>"
      ## Whether to use custom JDBC drivers. Set to true if you are using Oracle, PostgreSQL, or a special Db2 driver.
      use_custom_jdbc_drivers: false
      ## If use_custom_jdbc_drivers is set to true, input the name of the persistent volume claim (PVC) that binds to the persistent volume (PV) where the custom JDBC driver files are stored.
      ## If use_custom_jdbc_drivers is set to false, remove or comment out this parameter.
      custom_jdbc_pvc: "<Required>"
      ## The set of JDBC driver files.
      ## For DB2, it is normally "db2jcc4.jar db2jcc_license_cisuz.jar db2jcc_license_cu.jar".
      ## For Oracle, it is normally "ojdbc8.jar".
      ## For PostgreSQL, it is normally "postgresql-42.2.14.jar".
      jdbc_driver_files: 'db2jcc4.jar db2jcc_license_cisuz.jar db2jcc_license_cu.jar'
      ## Workflow Server database connect pool maximum number of physical connections.
      cm_max_pool_size: 200
      dbcheck:
        ## The maximum wait time (seconds) to check the database intialization status. The server fails to start after wait_time.
        wait_time: 900
        ## The interval time (seconds) to check the database initialization status before the database is ready and bootstrapped with system data.
        interval_time: 15
      hadr:
        ## Database standby host for high availability disaster recovery (HADR)
        ## To enable database HADR, configure both standby host and port.
        standbydb_host:
        ## Database standby port for HADR. To enable database HADR, configure both standby host and port.
        standbydb_port:
        ## Retry interval for HADR
        retryinterval:
        ## Maximum retries for HADR
        maxretries:

    ## The configurations for content integration for attachment in process
    content_integration:
      init_job_image:
        ## Image name for content integration container.
        repository: cp.icr.io/cp/cp4a/baw/iaws-ps-content-integration
        ## Image tag for content integration container.
        tag: 21.0.1-IF001
        ## Pull policy for content integration container. Default value is IfNotPresent. Possible values are IfNotPresent, Always.
        pull_policy: IfNotPresent
      ## Domain name for content integration. The value must be the same as initialize_configuration.ic_domain_creation.domain_name.
      domain_name: "P8DOMAIN"
      ## Object Store name for content integration.
      ## The value must be an existing object store in CPE.
      ## If use initialize_configuration for the object store initilization, the value must be one of initialize_configuration.ic_obj_store_creation.object_stores.
      object_store_name: "AWSINS1DOCS"
      ## Admin secret for connecting to Content Platform Engine (CPE). This parameter is optional. If not set, it will autodetect CPE's admin secret in the same namespace.
      cpe_admin_secret: ""

    ## Application engine configuration, because application engine is an array,
    ## when there is only one Application engine deployed along with this CR, below three parameters are not required.
    ## when there is more then one application engine deployed, below three parameters are required.
    appengine:
      ## App Engine hostname
      hostname: ""
      ## App Engine port
      port: "443"
      ## App Engine admin secret name
      admin_secret_name: ""


    ## The configuration for Java Messaging Service(JMS)
    jms:
      image:
        ## Image name for Java Messaging Service container.
        repository: cp.icr.io/cp/cp4a/baw/jms
        ## Image tag for Java Messaging Service container.
        tag: 21.0.1-IF001
        ## Pull policy for Java Messaging Service container. Default value is IfNotPresent. Possible values are IfNotPresent, Always.
        pull_policy: IfNotPresent
      tls:
        ## TLS secret name for Java Message Service (JMS)
        tls_secret_name: ibm-jms-tls-secret
      resources:
        limits:
          ## Memory limit for JMS configuration
          memory: "2Gi"
          ## CPU limit for JMS configuration
          cpu: "1000m"
        requests:
          ## Requested amount of memory for JMS configuration
          memory: "512Mi"
          ## Requested amount of CPU for JMS configuration
          cpu: "200m"
      storage:
        ## Whether to enable persistent storage for JMS
        persistent: true
        ## Size for JMS persistent storage
        size: "1Gi"
        ## Whether to enable dynamic provisioning for JMS persistent storage
        use_dynamic_provisioning: true
        ## Access modes for JMS persistent storage
        access_modes:
        - ReadWriteOnce
        ## Storage class name for JMS persistent storage
        storage_class: "{{ shared_configuration.storage_configuration.sc_fast_file_storage_classname }}"
      ## Default values for liveness probes. Modify these values to meet your requirements.
      liveness_probe:
        ## Number of seconds after the Workflow Server container starts before the liveness probe is initiated
        initial_delay_seconds: 180
        ## Number of seconds to wait before the next probe.
        period_seconds: 20
        ## Number of seconds after which the probe times out.
        timeout_seconds: 10
        ## When a probe fails, number of times that Kubernetes will try before giving up and restarting the container.
        failure_threshold: 3
        ## Minimum consecutive successes for the probe to be considered successful after it failed.
        success_threshold: 1
      ## Default values for rediness probes. Modify these values to meet your requirements.
      readiness_probe:
        ## Number of seconds after the Workflow Server container starts before the liveness probe is initiated
        initial_delay_seconds: 30
        ## Number of seconds to wait before the next probe.
        period_seconds: 5
        ## Number of seconds after which the probe times out.
        timeout_seconds: 5
        ## When a probe fails, number of times that Kubernetes will try before giving up and restarting the container.
        failure_threshold: 6
        ## Minimum consecutive successes for the probe to be considered successful after it failed.
        success_threshold: 1

    ## Resource configuration for init job
    resources_init:
      limits:
        ## CPU limit for init job containers.
        cpu: "500m"
        ## Memory limit for init job containers.
        memory: 256Mi
      requests:
        ## Requested amount of CPU for init job containers.
        cpu: "200m"
        ## Requested amount of memory for init job containers.
        memory: 128Mi

    ## Resource configuration for heavy init job such as database init job
    resources_init_heavy_job:
      limits:
        ## CPU limit for Workflow Server database init job container.
        cpu: 1
        ## Memory limit for Workflow Server database init job container.
        memory: 1536Mi
      requests:
        ## Requested amount of CPU for Workflow Server database init job container.
        cpu: "500m"
        ## Requested amount of memory for Workflow Server database init job container.
        memory: 512Mi

    ## Resource configuration
    resources:
      limits:
        ## CPU limit for Workflow Server.
        cpu: 4
        ## Memory limit for Workflow Server.
        memory: 3Gi
      requests:
        ## Requested amount of CPU for Workflow Server.
        cpu: "500m"
        ## Requested amount of memory for Workflow Server.
        memory: 2048Mi

    ## liveness and readiness probes configuration
    probe:
      ws:
        liveness_probe:
          ## Number of seconds after the Workflow Server container starts before the liveness probe is initiated
          initial_delay_seconds: 300
          ## Number of seconds to wait before the next probe.
          period_seconds: 10
          ## Number of seconds after which the probe times out.
          timeout_seconds: 10
          ## When a probe fails, number of times that Kubernetes will try before giving up and restarting the container.
          failure_threshold: 3
          ## Minimum consecutive successes for the probe to be considered successful after it failed.
          success_threshold: 1
        readinessProbe:
          ## Number of seconds after the Workflow Server container starts before the readiness probe is initiated
          initial_delay_seconds: 240
          ## Number of seconds to wait before the next probe.
          period_seconds: 5
          ## Number of seconds after which the probe times out.
          timeout_seconds: 5
          ## When a probe fails, number of times that Kubernetes will try before giving up and restarting the container.
          failure_threshold: 6
          ## Minimum consecutive successes for the probe to be considered successful after it failed.
          success_threshold: 1

    ## log trace configuration
    logs:
      ## Format for printing logs on the console.
      console_format: "json"
      ## Log level for printing logs on the console.
      console_log_level: "INFO"
      ## Source of the logs for printing on the console.
      console_source: "message,trace,accessLog,ffdc,audit"
      ## Required format for the messages.log file. Valid values are SIMPLE and JSON.
      message_format: "SIMPLE"
      ## Format of the trace log. Options values are: BASIC, ADVANCED, and ENHANCED.
      trace_format: "ENHANCED"
      ## Specification for printing trace logs.
      trace_specification: "*=info"
      # Maximum number of log files that are kept before the oldest file is removed.
      max_files: 10
      # The maximum size (in MB) that a log file can reach before it is rolled.
      max_filesize: 50

    ## storage configuration
    storage:
      ## Set to true to use dynamic storage provisioning. If set to false, you must set existing_pvc_for_logstore and existing_pvc_for_dumpstore.
      use_dynamic_provisioning: true
      ## Persistent volume claim (PVC) for logs.
      existing_pvc_for_logstore: ""
      ## Minimum size of the persistent volume (PV) that is mounted as the log store.
      size_for_logstore: "10Gi"
      ## Persistent volume claim (PVC) for dump files.
      existing_pvc_for_dumpstore: ""
      ## Minimum size of the persistent volume (PV) that is mounted as the dump store.
      size_for_dumpstore: "10Gi"
      ## Persistent volume claim (PVC) for generic files.
      existing_pvc_for_filestore: ""
      ## Minimum size of the persistent volume (PV) that is mounted as the generic file store.
      size_for_filestore: "10Gi"

    ## autoscaling
    autoscaling:
      ## Whether to enable automatically scaling the number of pods.
      enabled: false
      ## Upper limit for the number of pods that can be set by the autoscaler. If it is not specified or negative, the server uses the default value.
      max_replicas: 3
      ## Lower limit for the number of pods that can be set by the autoscaler. If it is not specified or negative, the server uses the default value.
      min_replicas: 2
      ## Target average CPU utilization (represented as a percent of requested CPU) over all the pods. If it is not specified or negative, the default is used.
      target_cpu_utilization_percentage: 80

    ## federation config
    federation_config:
      workflow_server:
          ## Number of primary shards of the Elasticsearch index used to store Workflow Server data.
          index_number_of_shards: 3
          ## Number of shard replicas of the Elasticsearch index used to store Workflow Server data.
          index_number_of_replicas: 1
    ## JVM options separated with spaces, for example: -Dtest1=test -Dtest2=test2.
    jvm_customize_options:

    ##  Workflow Server custom plain XML snippet
    ##  liberty_custom_xml: |+
    ##    <server>
    ##      <!-- custom propeties here -->
    ##    </server>
    liberty_custom_xml:

    ## Workflow Server custom XML secret name that contains custom configuration in Liberty server.xml,
    ## put the custom.xml in secret with key "sensitiveCustomConfig"
    ## kubectl create secret generic wfs-custom-xml-secret --from-file=sensitiveCustomConfig=./custom.xml
    custom_xml_secret_name:

    ## Workflow Server Lombardi custom XML secret name that contains custom configuration in 100Custom.xml
    ## put the 100Custom.xml in secret with key "sensitiveCustomConfig"
    #  kubectl create secret generic wfs-lombardi-custom-xml-secret --from-file=sensitiveCustomConfig=./100Custom.xml
    lombardi_custom_xml_secret_name:

  ########################################################################
  ########   IBM Process Federation Server configuration          ########
  ########################################################################
  pfs_configuration:
    ## If you input hostname and port here, they will always be used.
    ## If you are using pattern mode (the shared_configuration.sc_deployment_patterns contains value),
    ## you don't need to fill in the hostname and port: the shared_configuration.sc_deployment_hostname_suffix will be used to generate them.
    ## But if you didn't input a suffix, and no hostname port is assigned, an error will be reported in the operator log during deployment.
    ## For non pattern mode you must assign a valid hostname and port here.
    ## Process Federation Server hostname
    hostname: ""
    ## Process Federation Server port
    port: 443
    ## How the HTTPS endpoint service should be published. Possible values are ClusterIP, NodePort, Route
    service_type: Route

    ## The elasticsearch settings which will be used by PFS
    elasticsearch:
      ## Required only when using external elasticsearch. Endpoint of external elasticearch, such as: https://<external_es_host>:<external_es_port>.
      endpoint: ""
      ## Required only when using external elasticsearch. The external elasticsearch administrative secret that contains the keys: username, password and .htpasswd.
      admin_secret_name: ""
      ## The number of seconds for elasticsearch connection timeout setting.
      connect_timeout: 10s
      ## The number of seconds for elasticsearch read timeout setting.
      read_timeout: 30s
      ## elasticsearch thread count setting
      thread_count: 0
      ## The maximum number of connections allowed across all routes when PFS connects to the elasticsearch cluster to call its REST API.
      ## Specify a positive integer. If the provided value is less than or equal to 0, then the default Elasticsearch High Level REST Client value will be used.
      max_connection_total: -1
      ## The maximum number of connections allowed for a route when PFS connects to the elasticsearch cluster to call its REST API.
      ## Specify a positive integer. If the provided value is less than or equal to 0, then the default Elasticsearch High Level REST Client value will be used.
      max_connection_per_route: -1

    ## This is the image repository and tag that correspond to image registry, which is where the image will be pulled.
    image:
      ## Process Federation Server image
      ## The default repository is the IBM Entitled Registry.
      repository: cp.icr.io/cp/cp4a/baw/pfs-prod
      ## Process Federation Server image tag
      tag: "21.0.1-IF001"
      ## Process Federation Server image pull policy. This will override the image pull policy in the shared_configuration.
      pull_policy: IfNotPresent

    ## The number of initial Process Federation Server pods.
    replicas: 1
    ## Service account name for Process Federation Server pod. If not set, the default service account named "{{ meta.name }}-pfs-service-account" will be created.
    service_account:
    ## Whether Kubernetes can (soft) or must not (hard) deploy Process Federation Server pods onto the same node. Possible values are "soft" and "hard".
    anti_affinity: hard

    ## Whether to enable notification server. Possible values are: true and false
    enable_notification_server: false
    ## Whether to enable default security roles. Possible values are: true and false
    enable_default_security_roles: true
    
    ## Name of the secret containing the Process Federation Server administration passwords, such as ltpaPassword, oidcClientPassword, sslKeyPassword
    admin_secret_name: ibm-pfs-admin-secret
    ## Name of the secret containing the files that will be mounted in the /config/configDropins/overrides folder
    config_dropins_overrides_secret: ""
    ## Name of the secret containing the files that will be mounted in the /config/resources/security folder
    resources_security_secret: ""
    ## Name of the custom libraries containing the files that will be mounted in the /config/resources/libs folder
    custom_libs_pvc: ""

    ## The secret that contains the Transport Layer Security (TLS) key and certificate for external https visits. You can enter the secret name here.
    ## If you do not want to use the customized external TLS certificate, leave it empty.
    external_tls_secret:
    ## Certificate authority (CA) used to sign the external TLS secret. It is stored in the secret with the TLS key and certificate. You can enter the secret name here.
    ## If you don't want to use the customized CA to sign the external TLS certificate, leave it empty.
    external_tls_ca_secret:

    ## Whether to use the built-in monitoring capability.
    monitor_enabled: false

    tls:
      ## Existing TLS secret containing keys tls.key and tls.crt
      tls_secret_name:
      ## Existing TLS trust secret list
      ## You might specify a list of secrets, every secret stores a trusted CA
      ## kubectl create secret generic pfs_custom_trust_ca_secret1 --from-file=tls.crt=./ca1.crt
      tls_trust_list:
      ## The parameter is optional, if you want PFS server to trust your custom trusted CAs, you can add them to a keystore and then create a secret to store the trusted keystore.
      ## The keystore type must be JKS or PKCS12.
      ## kubectl create secret generic pfs_custom_trusted_keystore_secret --from-file=truststorefile=./trust.p12 --from-literal=type=PKCS12  --from-literal=password=WebAS
      tls_trust_store:

    ## The initial resources (CPU, memory) requests and limits.
    ## If more resources are needed, make the changes here to meet your requirement.
    resources:
      requests:
        ## The minimum amount of CPU required to start a PFS pod.
        cpu: 500m
        ## The minimum memory required (including JVM heap and file system cache) to start a PFS pod.
        memory: 512Mi
      limits:
        ## The maximum amount of CPU to allocate to each PFS pod.
        cpu: 2
        ## The maximum memory (including JVM heap and file system cache) to allocate to each PFS pod.
        memory: 4Gi

    ## Default values for liveness probes. Modify these values to meet your requirements.
    liveness_probe:
      ## Number of seconds after Process Federation Server container starts before the liveness probe is initiated
      initial_delay_seconds: 300
    
    ## Default values for readiness probes. Modify these values to meet your requirements.
    readiness_probe:
      ## Number of seconds after Process Federation Server container starts before the readiness probe is initiated
      initial_delay_seconds: 240

    ## Configurations for saved searches
    saved_searches:
      ## Name of the Elasticsearch index used to store saved searches.
      index_name: ibmpfssavedsearches
      ## Number of shards of the Elasticsearch index used to store saved searches.
      index_number_of_shards: 3
      ## Number of replicas (pods) of the Elasticsearch index used to store saved searches.
      index_number_of_replicas: 1
      ## Batch size used when retrieving saved searches.
      index_batch_size: 100
      ## Amount of time before considering an update lock as expired. Valid values are numbers with a trailing 'm' or 's' for minutes or seconds.
      update_lock_expiration: 5m
      ## Amount of time before considering a unique constraint as expired. Valid values are numbers with a trailing 'm' or 's' for minutes or seconds.
      unique_constraint_expiration: 5m

    security:
      sso:
        ## The ssoDomainNames property of the <webAppSecurity> tag.
        domain_name:
        ## The ssoCookieName property of the <webAppSecurity> tag.
        cookie_name: "ltpatoken2"
        ltpa:
          ## The keysFileName property of the <ltpa> tag.
          filename: "ltpa.keys"
          ## The expiration property of the <ltpa> tag.
          expiration: "120m"
          ## The monitorInterval property of the <ltpa> tag.
          monitor_interval: "60s"
      ## The sslProtocol property of the <ssl> tag used as default SSL config.
      ssl_protocol: SSL

    executor:
      ## Value of the maxThreads property of the <executor> tag.
      max_threads: "80"
      ## Value of the coreThreads property of the <executor> tag.
      core_threads: "40"

    rest:
      ## Value of the userGroupCheckInterval property of the <ibmPfs_restConfig> tag.
      user_group_check_interval: "300s"
      ## Value of the systemStatusCheckInterval property of the <ibmPfs_restConfig> tag.
      system_status_check_interval: "60s"
      ## Value of the bdFieldsCheckInterval property of the <ibmPfs_restConfig> tag.
      bd_fields_check_interval: "300s"

    custom_env_variables:
      ## Names of the custom environment variables defined in the secret referenced in pfs.customEnvVariables.secret
      names:
      # - name: MY_CUSTOM_ENVIRONMENT_VARIABLE
      ## Secret holding custom environment variables
      secret:

    ## Log trace configuration.
    logs:
      ## Format for printing logs on the console.
      console_format: "json"
      ## Log level for printing logs on the console.
      console_log_level: "INFO"
      ## Source of the logs for printing on the console.
      console_source: "message,trace,accessLog,ffdc,audit"
      ## The required format for the messages.log file. Valid values are SIMPLE or JSON format.
      message_format: "SIMPLE"
      ## Format for printing trace logs on the console.
      trace_format: "ENHANCED"
      ## Specification for printing trace logs.
      trace_specification: "*=info"
      storage:
        ## Use Dynamic Provisioning for PFS Logs Data Storage.
        use_dynamic_provisioning: true
        ## The minimum size of the persistent volume used mounted as PFS Liberty server /logs folder.
        size: 5Gi
        ## Storage class of the persistent volume used mounted as PFS Liberty server /logs folder.
        storage_class: "{{ shared_configuration.storage_configuration.sc_medium_file_storage_classname }}"
        ## The persistent volume claim for log data storage if dynamic provisioning is not used (use_dynamic_provisioning is false).
        existing_pvc_name: ""

    ## When PFS is deployed in an environment that includes the Resource Registry,
    ## the following additional parameters can be used to configure the integration between PFS and the Resource Registry.
    dba_resource_registry:
      ## Time to live of the lease that creates the PFS entry in the DBA Resource Registry, in seconds.
      lease_ttl: 120
      ## The interval at which to check that PFS is running, in seconds.
      pfs_check_interval: 10
      ## The number of seconds after which PFS will be considered as not running if no connection can be perfomed.
      pfs_connect_timeout: 10
      ## The number of seconds after which PFS will be considered as not running if has not yet responded.
      pfs_response_timeout: 30
      ## The key under which PFS should be registered in the DBA Service Registry when running.
      pfs_registration_key: /dba/appresources/IBM_PFS/PFS_SYSTEM
      ## The initial resources (CPU, memory) requests and limits. 
      ## If more resources are needed, make the changes here to meet your requirement.
      resources:
        limits:
          ## The maximum memory to allocate to each PFS registration pod.
          memory: '512Mi'
          ## The maximum amount of CPU to allocate to each PFS registration pod.
          cpu: '500m'
        requests:
          ## The minimum memory required to start a PFS registration pod.
          memory: '512Mi'
          ## The minimum amount of CPU required to start a PFS registration pod.
          cpu: '200m'

  ########################################################################
  ########   Embedded Elasticsearch configuration                 ########
  ########################################################################
  elasticsearch_configuration:
    es_image:
      ## Elasticsearch image
      repository: cp.icr.io/cp/cp4a/baw/pfs-elasticsearch-prod
      ## Elasticsearch image tag
      tag: "21.0.1-IF001"
      ## Elasticsearch image pull policy
      pull_policy: IfNotPresent
    es_init_image:
      ## The image used by the privileged init container to configure Elasticsearch system settings.
      ## This value is only relevant if elasticsearch_configuration.privileged is set to true
      repository: cp.icr.io/cp/cp4a/baw/pfs-init-prod
      ## The image tag for Elasticsearch init container
      tag: "21.0.1-IF001"
      ## The pull policy for Elasticsearch init container
      pull_policy: IfNotPresent
    es_nginx_image:
      ## The name of the Nginx docker image to be used by Elasticsearch pods
      repository: cp.icr.io/cp/cp4a/baw/pfs-nginx-prod
      ## The image tag of the Nginx docker image to be used by Elasticsearch pods
      tag: "21.0.1-IF001"
      ## The pull policy for the Nginx docker image to be used by Elasticsearch pods
      pull_policy: IfNotPresent

    ## Number of initial Elasticsearch pods
    replicas: 1
    ## How the HTTPS endpoint service should be published. The possible values are ClusterIP and NodePort
    service_type: ClusterIP
    ## The port to which the Elasticsearch server HTTPS endpoint will be exposed externally.
    ## This parameter is relevant only if elasticsearch_configuration.service_type is set to NodePort
    external_port:
    ## The elasticsearch admin secret that contains the username, password and .htpasswd.
    ## If not provided, the defualt admin secret named "{{ meta.name }}-elasticsearch-admin-secret" is used.
    admin_secret_name:
    ## Whether Kubernetes "may" (soft) or "must not" (hard) deploy Elasticsearch pods onto the same node
    ## The possible values are "soft" and "hard"
    anti_affinity: hard
    ## The Elasticsearch pods require the hosting worker nodes to be configured to:
    ## - disable memory swapping by setting the sysctl value vm.swappiness to 1.
    ## - increase the limit on the number of open files descriptors for the user running Elasticsearch by setting sysctl value vm.max_map_count to 65,536 or higher.
    ## When set to true, a privileged init container will execute the appropriate sysctl commands to update the worker node configuration to match Elasticsearch requirements.
    ## When set to false, you must ask the cluster administrator to change the memory swapping and descriptor properties on each worker node.
    privileged: false
    ## If elasticsearch_configuration.privileged is set to true, you must create a service account that has the privileged SecurityContextConstraint to allow running privileged containers. Refer to Knowledge Center for more info.
    ## If elasticsearch_configuration.service_account not set, default service account "{{ meta.name }}-elasticsearch-service-account" will be used.
    service_account: "<Required>"
    ## Initial delay for liveness and readiness probes of Elasticsearch pods
    probe_initial_delay: 90
    ## The JVM heap size to allocate to each Elasticsearch pod
    heap_size: "1024m"
    ## Whether to use the built-in monitoring capability.
    monitor_enabled: false

    ## The initial resources (CPU, memory) requests and limits. 
    ## If more resources are needed, make the changes here to meet your requirement.
    resources:
      requests:
        ## The minimum amount of CPU required to start a Elasticsearch pod.
        cpu: "100m"
        ## The minimum memory required (including JVM heap and file system cache) to start a Elasticsearch pod.
        memory: "2Gi"
      limits:
        ## The maximum amount of CPU to allocate to each Elasticsearch pod.
        cpu: "1000m"
        ## The maximum memory (including JVM heap and file system cache) to allocate to each Elasticsearch pod.
        memory: "2Gi"

    storage:
      ## If persistent the elasticsearch data. Set to false for non-production or trial-only deployment.
      persistent: true
      ## Set to true to use dynamic storage provisioner
      use_dynamic_provisioning: true
      ## The minimum size of the persistent volume
      size: 10Gi
      ## Storage class name for Elasticsearch persistent storage
      storage_class: "{{ shared_configuration.storage_configuration.sc_fast_file_storage_classname }}"

    snapshot_storage:
      ## If persistent the elasticsearch snapshot storage. Set to true for production deployment.
      enabled: false
      ## Set to true to use dynamic storage provisioner
      use_dynamic_provisioning: true
      ## The minimum size of the persistent volume
      size: 30Gi
      ## Storage class name for Elasticsearch persistent snapshot storage
      storage_class_name: ""
      ## By default, a new persistent volume claim is be created. Specify an existing claim here if one is available.
      existing_claim_name: ""

  ########################################################################
  ########  IBM FileNet Content Manager initialize configuration  ########
  ########################################################################
  ## The deployment of FNCM will be initialized with the default values assigned to the parameters below.
  ## The initialization process includes the creation of the P8 domain, the creation of the directory services,
  ## the assignments of users/groups to the P8 domain and object store(s), the creation of the object store(s),
  ## the creation/addition of add-ons for each object store, the enablement of workflow for each object store, the
  ## creation of Content Search Services servers, index areas, and the enabling of Content-based Retrieval (CBR) for each object store.
  ## In addition, the creation of Navigator desktop will also occur.
  ## If any of the values below does not fit your infrastructure, then change the value to correpond to your configuration
  ## (e.g., "CEAdmin" is the default user for ic_ldap_admin_user_name parameter and if you do not have "CEAdmin" user in your directory
  ## server and have a different user, then replace "CEAdmin" with your own user).  Otherwise, the rest of the values should remain as default.
  initialize_configuration:
    ic_domain_creation:
      ## Provide a name for the domain
      domain_name: "P8DOMAIN"
      ## The encryption strength
      encryption_key: "128"
    ic_ldap_creation:
      ## Administrator user
      ic_ldap_admin_user_name:
      - "CEAdmin"
      ## Administrator group
      ic_ldap_admins_groups_name:
      - "P8Administrators"
      ## Name of the LDAP directory
      ic_ldap_name: "ldap_name"
    ic_obj_store_creation:
      object_stores:
      ## Configuration for the document object store
      ## Display name for the document object store to create
      - oc_cpe_obj_store_display_name: "AWSINS1DOCS"
        ## Symbolic name for the document object store to create
        oc_cpe_obj_store_symb_name: "AWSINS1DOCS"
        oc_cpe_obj_store_conn:
          ## Object store connection name
          name: "DOCS_connection"
          ## The name of the site
          site_name: "InitialSite"
          ## Specify the name of the non-XA datasource (from dc_common_os_datasource_name in the dc_os_datasources section above)
          dc_os_datasource_name: "AWSINS1DOCS"
          ## The XA datasource
          dc_os_xa_datasource_name: "AWSINS1DOCSXA"
        ## Admin user group
        oc_cpe_obj_store_admin_user_groups:
        - "CEAdmin"
        ## An array of users with access to the object store
        oc_cpe_obj_store_basic_user_groups:
        ## Specify whether to enable add-ons
        oc_cpe_obj_store_addons: true
        ## Add-ons to enable for Content Platform Engine
        oc_cpe_obj_store_addons_list:
        - "{CE460ADD-0000-0000-0000-000000000004}"
        - "{CE460ADD-0000-0000-0000-000000000001}"
        - "{CE460ADD-0000-0000-0000-000000000003}"
        - "{CE460ADD-0000-0000-0000-000000000005}"
        - "{CE511ADD-0000-0000-0000-000000000006}"
        - "{CE460ADD-0000-0000-0000-000000000008}"
        - "{CE460ADD-0000-0000-0000-000000000007}"
        - "{CE460ADD-0000-0000-0000-000000000009}"
        - "{CE460ADD-0000-0000-0000-00000000000A}"
        - "{CE460ADD-0000-0000-0000-00000000000B}"
        - "{CE460ADD-0000-0000-0000-00000000000D}"
        - "{CE511ADD-0000-0000-0000-00000000000F}"
        ## Provide a name for the Advance Storage Area
        oc_cpe_obj_store_asa_name: "demo_storage"
        ## Provide a name for the file system storage device
        oc_cpe_obj_store_asa_file_systems_storage_device_name: "demo_file_system_storage"
        ## The root directory path for the object store storage area
        oc_cpe_obj_store_asa_root_dir_path: "/opt/ibm/asa/os01_storagearea"
        ## Specify whether to enable workflow for the object store
        oc_cpe_obj_store_enable_workflow: false
        ## Specify a name for the workflow region
        oc_cpe_obj_store_workflow_region_name: "docs_region_name"
        ## Specify the number of the workflow region
        oc_cpe_obj_store_workflow_region_number: 1
        ## Specify a table space for the workflow data
        oc_cpe_obj_store_workflow_data_tbl_space: "VWDATA_TS"
        ## Optionally specify a table space for the workflow index
        oc_cpe_obj_store_workflow_index_tbl_space: "VWINDEX_TS"
        ## Optionally specify a table space for the workflow blob.
        oc_cpe_obj_store_workflow_blob_tbl_space: "VWBLOB_TS"
        ## Designate an LDAP group for the workflow admin group.
        oc_cpe_obj_store_workflow_admin_group: "P8Administrators"
        ## Designate an LDAP group for the workflow config group
        oc_cpe_obj_store_workflow_config_group: "P8Administrators"
        ## Default format for date and time
        oc_cpe_obj_store_workflow_date_time_mask: "mm/dd/yy hh:tt am"
        ## Locale for the workflow
        oc_cpe_obj_store_workflow_locale: "en"
        ## Provide a name for the connection point
        oc_cpe_obj_store_workflow_pe_conn_point_name: ""
        # Enable the content event emitter only when deploying BAI.
        # Default value is false if not specified in the CR.
        oc_cpe_obj_store_enable_content_event_emitter: false

      ## Configuration for the application engine object store
      ## Display name for the application engine object store to create
      - oc_cpe_obj_store_display_name: "AEOS"
        ## Symbolic name for the application engine object store to create
        oc_cpe_obj_store_symb_name: "AEOS"
        oc_cpe_obj_store_conn:
          ## Object store connection name
          name: "AEOS_connection"
          ## The name of the site
          site_name: "InitialSite"
          ## Specify the name of the non-XA datasource (from dc_common_os_datasource_name in the dc_os_datasources section above)
          dc_os_datasource_name: "AEOS"
          ## The XA datasource
          dc_os_xa_datasource_name: "AEOSXA"
        ## Admin user group
        oc_cpe_obj_store_admin_user_groups:
        - "CEAdmin"
        ## An array of users with access to the object store
        oc_cpe_obj_store_basic_user_groups:
        ## Specify whether to enable add-ons
        oc_cpe_obj_store_addons: true
        ## Add-ons to enable for Content Platform Engine
        oc_cpe_obj_store_addons_list:
        - "{CE460ADD-0000-0000-0000-000000000004}"
        - "{CE460ADD-0000-0000-0000-000000000001}"
        - "{CE460ADD-0000-0000-0000-000000000003}"
        - "{CE460ADD-0000-0000-0000-000000000005}"
        - "{CE511ADD-0000-0000-0000-000000000006}"
        - "{CE460ADD-0000-0000-0000-000000000008}"
        - "{CE460ADD-0000-0000-0000-000000000007}"
        - "{CE460ADD-0000-0000-0000-000000000009}"
        - "{CE460ADD-0000-0000-0000-00000000000A}"
        - "{CE460ADD-0000-0000-0000-00000000000B}"
        - "{CE460ADD-0000-0000-0000-00000000000D}"
        - "{CE511ADD-0000-0000-0000-00000000000F}"
        ## Provide a name for the Advance Storage Area
        oc_cpe_obj_store_asa_name: "demo_storage"
        ## Provide a name for the file system storage device
        oc_cpe_obj_store_asa_file_systems_storage_device_name: "demo_file_system_storage"
        ## The root directory path for the object store storage area
        oc_cpe_obj_store_asa_root_dir_path: "/opt/ibm/asa/osae_storagearea"
        ## Specify whether to enable workflow for the object store
        oc_cpe_obj_store_enable_workflow: false
        ## Specify a name for the workflow region
        oc_cpe_obj_store_workflow_region_name: "aeos_region_name"
        ## Specify the number of the workflow region
        oc_cpe_obj_store_workflow_region_number: 1
        ## Specify a table space for the workflow data
        oc_cpe_obj_store_workflow_data_tbl_space: "VWDATA_TS"
        ## Optionally specify a table space for the workflow index
        oc_cpe_obj_store_workflow_index_tbl_space: "VWINDEX_TS"
        ## Optionally specify a table space for the workflow blob.
        oc_cpe_obj_store_workflow_blob_tbl_space: "VWBLOB_TS"
        ## Designate an LDAP group for the workflow admin group.
        oc_cpe_obj_store_workflow_admin_group: "P8Administrators"
        ## Designate an LDAP group for the workflow config group
        oc_cpe_obj_store_workflow_config_group: "P8Administrators"
        ## Default format for date and time
        oc_cpe_obj_store_workflow_date_time_mask: "mm/dd/yy hh:tt am"
        ## Locale for the workflow
        oc_cpe_obj_store_workflow_locale: "en"
        ## Provide a name for the connection point
        oc_cpe_obj_store_workflow_pe_conn_point_name: ""
        # Enable the content event emitter only when deploying BAI.
        # Default value is false if not specified in the CR.
        oc_cpe_obj_store_enable_content_event_emitter: false
